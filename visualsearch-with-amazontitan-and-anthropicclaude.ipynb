{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# Visual Search for Retail with Amazon Bedrock Titan Multimodal and Amazon OpenSearch Vector Databases\n",
    "\n",
    "This notebook will walk you through the process of building visual search functionality using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). We will use an Embeddings Model hosted on Amazon Bedrock to convert rproduct images and descriptions to vectors and store and search them in an [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook is tested on <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> and within <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html\">Amazon SageMaker Studio Notebook</a> and within an AWS Region that supports <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\">Amazon OpenSearch Serverless</a>.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.m5.xlarge</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 3</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Pre-requisites](#pre-requisites)\n",
    "\n",
    "    1.a [Check and configure access to the Internet](#1.a)\n",
    "\n",
    "    1.b [Install required software libraries](#1.b)\n",
    "    \n",
    "    1.c. [Configure logging](#1.c)\n",
    "    \n",
    "    1.d. [Organize imports](#1.d)\n",
    "    \n",
    "    1.e. [Set AWS Region and boto3 config](#1.e)\n",
    "    \n",
    "    1.f. [Check and create an Amazon OpenSearch Serverless collection](#1.f)\n",
    "    \n",
    "    1.g. [Enable model access in Amazon Bedrock](#1.g)\n",
    "    \n",
    "    1.h. [Check and configure security permissions](#1.h)\n",
    "\n",
    "    1.i. [Create common objects](#1.i)\n",
    "    \n",
    "    1.j. [Create an index in the Amazon OpenSearch Serverless collection](#1.j)\n",
    "    \n",
    " 2. [Create the OpenSearch Search Functionality ](#2)\n",
    " \n",
    "    2.a. [Prepare to load data into the vector database](#3.a)        \n",
    "        \n",
    "    2.b. [Create the embeddings](#3.b)\n",
    "    \n",
    "    2.c. [Store the embeddings in the vector database](#3.c)\n",
    "    \n",
    "    2.d. [Test searching our vector database](#3.d)\n",
    "    \n",
    "    2.e. [Build our search functions](#3.e)\n",
    "  \n",
    " 3. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  1. Pre-requisites <a id ='pre-requisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###  1.a. Check and configure access to the Internet <a id ='1.a'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.b. Install required software libraries <a id ='1.b'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [LangChain](https://www.langchain.com/)\n",
    "* [OpenSearch Python Client](https://pypi.org/project/opensearch-py/)\n",
    "* [Tqdm](https://pypi.org/project/tqdm/)\n",
    "* [Backoff](https://pypi.org/project/backoff/)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq pip\n",
    "%pip install -Uq boto3\n",
    "%pip install -q langchain==0.0.339\n",
    "%pip install -q opensearch-py==2.4.2\n",
    "%pip install -q tqdm==4.66.1\n",
    "%pip install -q backoff\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.c. Configure logging <a id ='1.c'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  a. System logs <a id='Configure%20system%20logs'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "Run the following cell to print the name of the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "logging.getLogger('sagemaker.config').setLevel(logging.CRITICAL)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.d. Organize imports <a id ='1.d'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import langchain\n",
    "import opensearchpy\n",
    "import requests\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from IPython.core.display import HTML\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from urllib.parse import urlparse\n",
    "import base64\n",
    "import backoff\n",
    "import secrets\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "from opensearchpy.helpers import parallel_bulk\n",
    "import re\n",
    "import unicodedata\n",
    "import concurrent.futures as cf\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from opensearchpy.helpers import parallel_bulk\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from os.path import splitext\n",
    "from botocore.exceptions import ClientError\n",
    "import io\n",
    "\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n",
    "    '''print(\n",
    "        \"Backing off {wait:0.1f} seconds after {tries} tries \"\n",
    "        \"calling function {target} with kwargs \"\n",
    "        \"{kwargs}\".format(**details)\n",
    "    )'''\n",
    "    \n",
    "def secure_randint(a, b):\n",
    "    return a + secrets.randbelow(b - a + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "logging.info(\"LangChain version : {}\".format(langchain.__version__))\n",
    "logging.info(\"OpenSearch Python Client version : {}\".format(opensearchpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.e. Set AWS Region and boto3 config <a id ='1.e'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. This will be used to initiate some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7632923",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock will use the current AWS Region. For Bedrock, follow the guidance in the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-west-2 (Oregon)</mark> for Amazon Bedrock.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037155d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 600,\n",
    "        'mode': 'adaptive'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3260b",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.f. Check and create an Amazon OpenSearch Serverless collection <a id ='1.f'> </a>\n",
    "\n",
    "This notebook uses an [Amazon OpenSearch Serverless (AOSS) collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-collections.html) of type [Vector search](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-usecase) as the vector database that will be used by the chat assistant.\n",
    "\n",
    "Run the following cells to check and create an AOSS collection if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b06917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the flags to identify if an AOSS collection exists and if it is created through this notebook\n",
    "aoss_collection_exists = False\n",
    "aoss_collection_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687632",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For the purpose of running this notebook, it is preferable to have an empty collection.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcfd03",
   "metadata": {},
   "source": [
    "Run the following code cell to retreive the details of the first available AOSS collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acac2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the AOSS client\n",
    "aoss_client = boto3.client(\"opensearchserverless\", config = my_boto3_config)\n",
    "\n",
    "# Check and create a collection if none is found\n",
    "collection_id = ''\n",
    "collections = aoss_client.list_collections()['collectionSummaries']\n",
    "if len(collections) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"Found an AOSS collection.\")\n",
    "    first_collection = collections[0]\n",
    "    collection_id = first_collection[\"id\"]\n",
    "    collection_name = first_collection[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31718d03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you like to create an AOSS collection through this notebook, then, run the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d5c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Note: It may take 8 to 10 minutes to create the AOSS collection.\n",
    "\n",
    "# The helper function 'create_aoss_collection' (available through ./scripts/helper_functions.py) creates the specified\n",
    "# AOSS collection with the following policies:\n",
    "# Data access policy: provides full access to the IAM role associated with this notebook instance.\n",
    "# Encryption policy: encrypts with AWS owned key.\n",
    "# Network policy: provides public network access to the collection.\n",
    "if aoss_collection_exists:\n",
    "    logging.info(\"Skipping AOSS collection creation.\")\n",
    "else:\n",
    "    collection_name = \"vs-collection\"\n",
    "    data_access_policy_name = \"vs-collection-dap\"\n",
    "    encryption_policy_name = \"vs-collection-ep\"\n",
    "    network_policy_name = \"vs-collection-np\"\n",
    "    response = create_aoss_collection(aoss_client, collection_name, data_access_policy_name,\n",
    "                                      encryption_policy_name, network_policy_name, my_iam_role)\n",
    "    collection_id = response[\"id\"]\n",
    "    collection_name = response[\"name\"]\n",
    "    aoss_collection_created = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb9a28",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the AOSS collection that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572f405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(collection_id) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"The following AOSS collection will be used:\\nCollection id: {}; Collection name: {}\"\n",
    "                 .format(collection_id, collection_name))\n",
    "    # Print the AWS console URL to the AOSS collection\n",
    "    collection_aws_console_url = \"https://{}.console.aws.amazon.com/aos/home?region={}#opensearch/collections/{}\"\\\n",
    "    .format(my_region, my_region, collection_name)\n",
    "    logging.info(\"If you like to take a look at this collection, visit {}\".format(collection_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.g. Enable model access in Amazon Bedrock <a id ='1.g'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84ba9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> Before invoking any model in Amazon Bedrock, enable access to that model by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. In addition, for Anthropic models, you need to submit the use case details. Otherwise, you will get an authorization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de160b",
   "metadata": {},
   "source": [
    "Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the Amazon Bedrock model access page URL\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you explicitly disable it, this is a one-time setup for each model in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.h. Check and configure security permissions <a id ='1.h'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "This IAM role should have the following permissions,\n",
    "\n",
    "1. Access to invoke the Foundation Models, you are using on Amazon Bedrock.\n",
    "2. Full access to read and write to the Amazon OpenSearch Serverless collection created in the previous step.\n",
    "3. Access to write to Amazon CloudWatch Logs.\n",
    "\n",
    "In addition, [data access control](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html) should be setup on the Amazon OpenSearch Serverless collection to provide create, read and write access to the IAM role associated with this notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.i. Create common objects <a id='1.i'></a>\n",
    "\n",
    "To begin with, list all the available models in Amazon Bedrock by running the following cell. This will help you pick a LLM and the Embeddings model within Amazon Bedrock that you will be using in this notebook. By default, both will use the On-Demand pricing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e08e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all the available foundation models in Amazon Bedrock\n",
    "models_info = ''\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "response = bedrock_client.list_foundation_models()\n",
    "model_summaries = response[\"modelSummaries\"]\n",
    "models_info = models_info + \"\\n\"\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(\"Provider Name\", \"Model Name\", \"Input Modalities\",\n",
    "                                                          \"Output Modalities\", \"Model Id\")\n",
    "models_info = models_info + \"-\".ljust(125, \"-\")\n",
    "for model_summary in model_summaries:\n",
    "    models_info = models_info + \"\\n\"\n",
    "    models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(model_summary[\"providerName\"],\n",
    "                                                                            model_summary[\"modelName\"],\n",
    "                                                                            \"|\".join(model_summary[\"inputModalities\"]),\n",
    "                                                                            \"|\".join(model_summary[\"outputModalities\"]),\n",
    "                                                                            model_summary[\"modelId\"])\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "logging.info(\"Displaying available models in the '{}' Region:\".format(br_region) + models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a835e",
   "metadata": {},
   "source": [
    "From the results of running the above cell,\n",
    "\n",
    "1. Pick the model-id that corresponds to the LLM that you want and set it as the value of the `llm_model_id` variable in the following cell.\n",
    "2. (Optional) Specify the [LLM-specific inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) in the `model_kwargs` parameter.\n",
    "2. Pick the model-id that corresponds to the Embeddings model that you want and set it as the value of the `embeddings_model_id` variable in the following cell.\n",
    "\n",
    "Now, run the following cell to create the common objects to be used in future steps in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e64ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook was tested with the following Amazon Bedrock models:\n",
    "    <li>Embedding model(s): amazon.titan-embed-image-v1</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb830d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model-id of the Embeddings model to be used to generate embeddings\n",
    "embeddings_model_id = \"amazon.titan-embed-image-v1\"\n",
    "\n",
    "##### LLM related objects\n",
    "# Function to initialise the Amazon Bedrock runtime client\n",
    "def initialise_bedrock():\n",
    "    return boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "##### Embeddings related objects\n",
    "# Function to create the Embeddings client using the LangChain BedrockEmbeddings class\n",
    "bedrock = initialise_bedrock()\n",
    "def create_embeddings_client():\n",
    "    return BedrockEmbeddings(client = bedrock, model_id = embeddings_model_id, region_name = br_region)\n",
    "\n",
    "br_embeddings = create_embeddings_client()\n",
    "\n",
    "\n",
    "#Function to initialise the Amazon S3 Client\n",
    "def initialise_s3():\n",
    "    return boto3.resource('s3')\n",
    "\n",
    "s3 = initialise_s3()\n",
    "sage_sess = sagemaker.Session()\n",
    "default_bucket = sage_sess.default_bucket()\n",
    "\n",
    "##### Amazon OpenSearch Serverless (AOSS) related objects\n",
    "# Create the AOSS Python client from the AOSS boto3 client using the helper function \n",
    "# available through ./scripts/helper_functions.py)\n",
    "aoss_py_client = auth_opensearch(host = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "                            service = 'aoss', region = my_region)\n",
    "# Specify the name of the index in the AOSS collection; this will be created later in the notebook\n",
    "index_name = \"product-embeddings-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4b73",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  1.j. Create an index in the Amazon OpenSearch Serverless collection <a id='1.j'></a>\n",
    "\n",
    "To create an index in the Amazon OpenSearch Serverless (AOSS) collection, we first need to define a schema for our index. AOSS allows users to specify a simple search index, which utilizes keyword matching, or the vector search feature, which utilizes [k-Nearest Neighbor (k-NN) search](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html). Vector search differs from standard search in that instead of using a typical keyword matching or fuzzy matching algorithm, vector search compares [embeddings](https://en.wikipedia.org/wiki/Word_embedding) of two pieces of text. An embedding is a numerical representation of a piece of information, like text, that we can compare against other embeddings. To learn more about embeddings, take a look at [this blog](https://huggingface.co/blog/getting-started-with-embeddings). The vector search feature allows us to search for documents that are semantically similar to the questions that our end users send to our chat assistant. This can improve the context that we then give to our LLM to answer the user's questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ff129-c6f3-484e-bf8c-15269e10f822",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the schema for the index with an k-NN type vector as the embedding\n",
    "hnsw_index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 512\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "        \"product_image_and_description_embedding\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1024,\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",\n",
    "              \"engine\": \"nmslib\",\n",
    "              \"space_type\": \"cosinesimil\",\n",
    "              \"parameters\": {\n",
    "                \"ef_construction\": 512,\n",
    "                \"m\": 16\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"prodId\": {\n",
    "            \"type\": \"text\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "# Create the index if it does not exist\n",
    "if aoss_py_client.indices.exists(index = index_name):\n",
    "    logging.info(\"AOSS index '{}' already exists.\".format(index_name))\n",
    "else:\n",
    "    logging.info(\"Creating AOSS index '{}'...\".format(index_name))\n",
    "    logging.info(aoss_py_client.indices.create(index = index_name, body = hnsw_index_body, ignore = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e2d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the AWS console URL to the AOSS index\n",
    "index_aws_console_url = collection_aws_console_url + \"/\" + index_name\n",
    "logging.info(\"If you like to take a look at this index, visit {}\".format(index_aws_console_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f6bb6-8ea5-4f33-abeb-e658e6403209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read product items from product_items.json\n",
    "accumulated_lines = \"\"\n",
    "with open('product_items.json', 'r') as json_file:\n",
    "    for line in json_file:\n",
    "        accumulated_lines += line.strip()\n",
    "    product_items = json.loads(accumulated_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121b20b-99e7-4915-a40f-854942e9a5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to create a claude prompt to describe an image that uses anthropic claude's messages api\n",
    "def generate_claude_search_prompt(image_data):\n",
    "    system = (\n",
    "        \"You are a merchandizer expert in writing product descriptions. Your task is to generate a \"\n",
    "        \"detailed description of input images for effective visual search. Your \"\n",
    "        \"description should be engaging, informative, and detailed enough to facilitate\"\n",
    "        \"effective multimodal embedding for \"\n",
    "        \"similarity search. Aim for clarity and precision to ensure the generated \"\n",
    "        \"embeddings capture the essence of the product accurately.\"\n",
    "    )\n",
    "    \n",
    "    prompt = (\n",
    "        \"Given this input image, generate a detailed and comprehensive description \"\n",
    "        \"that includes the following aspects:\\n\\n\"\n",
    "        \"1. Physical Description: Describe the physical attributes of the product, \"\n",
    "        \"including color, shape, size, material, and any distinctive features or \"\n",
    "        \"design elements. Mention textures, patterns, or any aesthetic \"\n",
    "        \"characteristics that stand out.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": image_data \n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"system\": system,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    return body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5d414",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create the OpenSearch Search Functionality <a id ='2'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac8ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.a Prepare to load data into the vector database <a id='3.a'></a>\n",
    "\n",
    "An Amazon OpenSearch Serverless (AOSS) collection is a logical grouping of one or more indexes that work together to support a specific workload or use case.\n",
    "\n",
    "This notebook will use a vector index for indexing documents in the AOSS collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ed1c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Initialize the text splitter <a id='Initialize%20the%20text%20splitter'></a>\n",
    "\n",
    "When we are running inference on text descriptions, providing an entire description to a LLM as context can be overwhelming to our LLM, especially for very long descriptions. A best practice is to divide the text into easier to consume partially overlapping chunks.\n",
    "\n",
    "Let's use the LangChain's [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) to create a text splitting object that we will use split the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187eb0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7338f-4e5c-4c1e-ba3a-9f62c2d3202a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to get product id\n",
    "def get_id(product_data: dict) -> str:\n",
    "    return product_data.get('pid', '')\n",
    "\n",
    "# function to get urls of product images\n",
    "def get_image_url(product_data: dict) -> str:\n",
    "    return product_data.get('image_url', '')\n",
    "\n",
    "# function to get augmented desccription\n",
    "def get_augmented_description(product_data: dict) -> str:\n",
    "    return product_data.get('product_description', '')\n",
    "\n",
    "# function to get relevant values for embeddings from products\n",
    "def process_product_item(product_data: dict) -> Tuple:\n",
    "    return get_id(product_data), get_image_url(product_data), get_augmented_description(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6e860-d002-47e7-84b2-c1c54f491c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_product_hash_map(product_items):\n",
    "    \"\"\"\n",
    "    Create a hash map mapping the 'id' of each product item to the product item itself.\n",
    "\n",
    "    Parameters:\n",
    "    product_items (list): A list of dictionaries, each representing a product item.\n",
    "\n",
    "    Returns:\n",
    "    dict: A hash map where each key is an 'id' from the product items and each value is the corresponding product item.\n",
    "    \"\"\"\n",
    "    hash_map = {}\n",
    "    for item in product_items:\n",
    "        product_id = item.get('pid')\n",
    "        if product_id:\n",
    "            hash_map[product_id] = item\n",
    "    return hash_map\n",
    "\n",
    "product_items_map = create_product_hash_map(product_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157d8a0",
   "metadata": {},
   "source": [
    "Now we have all our products with their respective images hosted on s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12b335-5d79-45a7-b4d2-b487fefd6f64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###  2.b. Create the embeddings <a id='3.b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84286731-eb1e-4185-aea8-aa6401635dad",
   "metadata": {},
   "source": [
    "Let us set up our functions to generate embeddings by calling the bedrock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabb2ca-0b44-4db0-844f-3b4f3d66459f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_image(url:str,s3):\n",
    "    bucket_name, key = url.replace('s3://', '').split('/', 1)\n",
    "\n",
    "    obj = s3.Object(bucket_name, key)\n",
    "    \n",
    "    try:\n",
    "        image_data = obj.get()['Body'].read()\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        image.thumbnail((1092,1092))\n",
    "\n",
    "        # Convert image to base64\n",
    "        buffered = BytesIO()\n",
    "        image_format = image.format if image.format else 'JPEG' \n",
    "        image.save(buffered, format=image_format)\n",
    "        return base64.b64encode(buffered.getvalue()).decode(), True\n",
    "    except Exception as e:\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e37392-7803-4c64-b074-3c525b6cb840",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility function to generate text embedding or image embedding in isolation, if you just have text or image as input.\n",
    "'''def get_image_or_text_embedding(image_data:Optional = None, text_chunks:Optional = None, bedrock=None):\n",
    "    modelId = \"amazon.titan-embed-image-v1\"\n",
    "    contentType = \"application/json\"\n",
    "    accept = \"application/json\"\n",
    "    \n",
    "    if not text_chunks:\n",
    "        body = json.dumps({\"inputImage\": image_data})\n",
    "        response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "        return json.loads(response.get('body').read())['embedding']\n",
    "    \n",
    "    chunk_embeddings = []\n",
    "    \n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        body = json.dumps({\"inputText\": chunk})\n",
    "        response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        chunk_embedding = json.loads(response.get('body').read())['embedding']\n",
    "        chunk_embeddings.append(chunk_embedding)\n",
    "        \n",
    "    return aggregate_embeddings(chunk_embeddings)'''\n",
    "\n",
    "# Function to generate a multimodal embedding using both text and image. \n",
    "# It is recommended to use the multi-modal embedding for better search results\n",
    "def get_multi_embedding(image_data, text_chunks, bedrock):\n",
    "    modelId=\"amazon.titan-embed-image-v1\"\n",
    "    contentType = \"application/json\"\n",
    "    accept = \"application/json\"\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        body = json.dumps({\"inputText\": chunk,\n",
    "                           \"inputImage\": image_data})\n",
    "\n",
    "        response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "        chunk_embedding = json.loads(response.get('body').read())['embedding']\n",
    "        embeddings.append(chunk_embedding)\n",
    "\n",
    "    aggregated_embedding = aggregate_embeddings(embeddings)\n",
    "\n",
    "    return aggregated_embedding\n",
    "\n",
    "def normalise_embedding(embedding):\n",
    "    return embedding / np.linalg.norm(embedding)\n",
    "\n",
    "def aggregate_embeddings(embeddings):\n",
    "    averaged_embedding = [sum(x)/len(embeddings) for x in zip(*embeddings)]\n",
    "    return averaged_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1822b-0eee-407b-a838-845c611245f4",
   "metadata": {},
   "source": [
    "Let's also create a function to sanitise our text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe280e4c-7b4c-4a6a-878d-d67cd6701bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to sanitise text descriptions\n",
    "def clean_and_prepare_text(text):\n",
    "    # Normalize extra whitespace to single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0fda08-b26c-4d87-be25-c849e2c5fbda",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> Below 3 cells are optional. <br/>\n",
    "Now that we have our helper functions to compute embeddings let us test them out for a product and monitor the outputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277c23f-4054-4679-b189-1fea881a7b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_id, image_url, product_description = process_product_item(product_items[0])\n",
    "image_response = fetch_image(image_url,s3)\n",
    "image_content = image_response[0]\n",
    "image_response[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce48dce-4124-40bd-b476-9016b628d02a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_product_item(product_items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e224ba-0a99-4421-b6a0-b66c0d9a60a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multi_embedding = np.array(get_multi_embedding(image_content,text_splitter.split_text(clean_and_prepare_text(product_description)),bedrock))\n",
    "multi_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7f5d2-b094-4beb-8bb0-d7aad4b9db61",
   "metadata": {},
   "source": [
    "Now for each product image we will generate an image embedding, its corresponding text description embedding and a multimodal embedding leveraging both the image and text description. We will then use these to create actions\n",
    "we can use to bulk load these embeddings into our OpenSearch vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e609a0a-a05b-45c5-b75f-d163e16ad027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to generate load actions from product items\n",
    "def process_records_to_actions_partial(items_chunk):\n",
    "    bedrock = initialise_bedrock()\n",
    "    s3 = initialise_s3()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1400,\n",
    "        chunk_overlap  = 600,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False,\n",
    "        )\n",
    "    \n",
    "    local_actions = []\n",
    "    \n",
    "    for item in items_chunk:\n",
    "            product_id, image_url, description = process_product_item(item)\n",
    "            image_response = fetch_image(image_url,s3)\n",
    "            if not image_response[1]:\n",
    "                    continue\n",
    "            try:\n",
    "                multi_embedding = normalise_embedding(np.array(get_multi_embedding(image_data=image_response[0],\n",
    "                                                                                   text_chunks=text_splitter.split_text(description),\n",
    "                                                                                   bedrock=bedrock)))\n",
    "                load_action = {\n",
    "                    \"_index\": \"product-embeddings-index\",\n",
    "                    \"_source\":{\n",
    "                        \"prodId\": product_id,\n",
    "                        \"product_image_and_description_embedding\": multi_embedding.tolist()\n",
    "                    }\n",
    "                }\n",
    "                local_actions.append(load_action)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return local_actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb822a4-276b-4768-a6b9-4191c0852691",
   "metadata": {},
   "source": [
    "now that we have our function to create load actions let's generate these actions for each product image, we will parallelise this for speed and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb32d53-e97c-4a05-95de-a61f4b380fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(data, num_splits):\n",
    "    avg = len(data) // num_splits\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(data):\n",
    "        out.append(data[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a284f-ba53-4b51-b5de-7476056cbbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to handle the progress update\n",
    "def update_progress(future, pbar):\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0d756",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> After executing the below cell, it may take up to few minutes to complete (timing depends on the data volumes). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c35590-d397-4dfd-9ef1-4a86ed224aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_PROCESSES = 2\n",
    "items_chunks = split_data(product_items, NUM_PROCESSES)\n",
    "main_pbar = tqdm(total=NUM_PROCESSES, desc='Overall Progress')\n",
    "\n",
    "# Using ThreadPoolExecutor to process records\n",
    "with cf.ThreadPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "    # Submitting tasks and attaching a callback to update the main progress bar\n",
    "    future_to_chunk = {executor.submit(process_records_to_actions_partial, chunk): chunk for chunk in items_chunks}\n",
    "    for future in cf.as_completed(future_to_chunk):\n",
    "        future.add_done_callback(lambda f: update_progress(f, main_pbar))\n",
    "\n",
    "# Ensure the main progress bar is closed\n",
    "main_pbar.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1801276",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.c Store the embeddings in the vector database <a id='3.c'></a>\n",
    "\n",
    "Run the following cells to upload the prepared documents into our created AOSS collection's index. The below function uses a parallel processing function to upload our documents into our index. The number of parallel worker threads is controlled by the `thread_count` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74144a3-f418-400f-9ede-67f89dff14cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize global arrays to get all documents from previous threads in one array\n",
    "global_actions = []\n",
    "\n",
    "for future in future_to_chunk:\n",
    "    local_actions = future.result()\n",
    "\n",
    "    global_actions.extend(local_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f253900-fca6-41e0-93f5-f53a86f55f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(global_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1b9d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the time of writing this notebook, AOSS did not support ingestion with <i>id</i> for <i>Vector search</i> collection type. As a result, running the following cell more than once will result in duplicate documents being created in the AOSS index. This is ok for the purpose of running this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec74d7-cc24-4c8a-b4eb-4cd0b2433ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load documents into our index using opensearh-py's parallel_bulk\n",
    "for success, info in parallel_bulk(client=aoss_py_client,actions=global_actions, request_timeout=60*60, thread_count = 8):\n",
    "    if not success:\n",
    "        print('A document failed:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a87aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.d Test searching our vector database <a id='3.d'></a>\n",
    "\n",
    "\n",
    "Now let's test a search on our product_image_embedding field with the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fef8db-83f0-4faf-a6db-4fff322c2cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_image(img_file, size=(1092, 1092)):\n",
    "    with Image.open(img_file) as img:\n",
    "        img.thumbnail(size)\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\")\n",
    "        return buffer.getvalue()\n",
    "\n",
    "def encode_image(img_file):\n",
    "    resized_image = resize_image(img_file)\n",
    "    img_str = base64.b64encode(resized_image)\n",
    "    base64_string = img_str.decode(\"latin1\")\n",
    "    return base64_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8248891-220f-4ffc-a47d-233cb24feef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base64_image(image_path:str)->str:\n",
    "    if 's3' in image_path:\n",
    "        s3 = initialise_s3()\n",
    "        return fetch_image(image_path,s3)[0]\n",
    "    else:\n",
    "        return encode_image(image_path)\n",
    "    return ''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0e485-0ca2-4189-9dd4-b0b014e9b962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (ClientError),\n",
    "        max_time=secure_randint(500, 1000),\n",
    "        on_backoff=backoff_hdlr,\n",
    "        giveup=lambda e: 'ThrottlingException' not in str(e) and 'ReadTimeoutError' not in str(e) and 'ModelTimeoutException' not in str(e)\n",
    "    ) \n",
    "def get_search_description(base64_image:str, bedrock=None)->str:\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    contentType = \"application/json\"\n",
    "    accept = \"application/json\"\n",
    "    \n",
    "    body = json.dumps(generate_claude_search_prompt(base64_image))\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    return json.loads(response.get('body').read())['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23149f51-17d6-49a4-ae58-93134adac99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace /images/test_image.jpeg with your test image path \n",
    "query_image_path = \"/images/test_image.jpeg\"\n",
    "get_search_description(get_base64_image(image_path=query_image_path),bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e156213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=30\n",
    "base64_image = get_base64_image(image_path=query_image_path)\n",
    "search_query = {\n",
    "        \"size\": k,\n",
    "        \"_source\": [\"prodId\"], #to only Include only the product ID field in the response\n",
    "        \"query\":{\n",
    "            \"knn\": {\n",
    "                \"product_image_and_description_embedding\": {\n",
    "                    \"vector\": get_multi_embedding(image_data=base64_image,\n",
    "                                                  text_chunks=text_splitter.split_text(clean_and_prepare_text(get_search_description(base64_image,bedrock))),\n",
    "                                                  bedrock=bedrock),\n",
    "                    \"k\": k,\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "aoss_py_client.search(index=\"product-embeddings-index\", body=search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c5885-3e6f-4469-a646-ab090af34aa5",
   "metadata": {},
   "source": [
    "And to visualise our results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ae73-83c0-47f8-96e6-89548f438f86",
   "metadata": {},
   "source": [
    "We retrieve the relevant information we want from the results, the product IDs and their scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4290cf1-65b0-4931-a0ea-5e9d156a2930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = [(item['_source']['prodId'],item['_score']) for item in aoss_py_client.search(index=\"product-embeddings-index\", body=search_query)[\"hits\"][\"hits\"]]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7495b2-4d89-4693-881d-7e11be6af20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_and_sort_scores(results):\n",
    "    score_dict = {}\n",
    "    for prod_id, score in results:\n",
    "        if prod_id in score_dict:\n",
    "            score_dict[prod_id] += score\n",
    "        else:\n",
    "            score_dict[prod_id] = score\n",
    "\n",
    "    sorted_prod_ids = sorted(score_dict, key=score_dict.get, reverse=True)\n",
    "\n",
    "    return sorted_prod_ids\n",
    "sorted_prod_ids = aggregate_and_sort_scores(results)\n",
    "print(sorted_prod_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3ee7e-fb63-49c2-b1a7-6aca63b7ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_images_from_s3_with_fetch_image(products, s3):\n",
    "    for i, result in enumerate(products, 2):\n",
    "        image_data_base64, success = fetch_image(result[1], s3)\n",
    "        if not success:\n",
    "            print(f\"Failed to fetch image for ID: {result[0]}\")\n",
    "            continue\n",
    "\n",
    "        # Decode the base64 image data\n",
    "        image_data = base64.b64decode(image_data_base64)\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(1, len(products) + 1, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"ID: {result[0][:8]}\")\n",
    "        plt.axis('off')  # Optional: to hide axes for a cleaner display\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b937ea-5a2a-4d4c-9293-5ab930f63ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "products = []\n",
    "for product_id in sorted_prod_ids[:5]:\n",
    "    item = product_items_map.get(product_id)\n",
    "    images=item['image_url']\n",
    "    products.append((item['product_name'],images))\n",
    "    \n",
    "query_image = Image.open(query_image_path,)  \n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, len(products) + 1, 1)\n",
    "plt.imshow(query_image)\n",
    "plt.title(\"Query\")\n",
    "plt.axis('off')\n",
    "        \n",
    "# Display Search Results\n",
    "for i, result in enumerate(products, 2):\n",
    "    image_data_base64, success = fetch_image(result[1], s3)\n",
    "    if not success:\n",
    "        print(f\"Failed to fetch image for ID: {result[0]}\")\n",
    "        continue\n",
    "        \n",
    "    # Decode the base64 image data\n",
    "    image_data = base64.b64decode(image_data_base64)\n",
    "    img = Image.open(BytesIO(image_data))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.subplot(1, len(products) + 1, i)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"ID: {result[0]}\")\n",
    "    plt.axis('off')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1263fcf-69e2-4282-8ae0-3411f9fae876",
   "metadata": {},
   "source": [
    "Now that we have our product emebeddings in our OpenSearch index, let's create some functions to allow us to search it, leveraging all the fields we have in our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905ed7-b6a9-416d-9063-18716cc5b8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def search_opensearch_indexes(query_image_url:Optional[str]=None, k:Optional[int]=5):\n",
    "    \"\"\"\n",
    "    Search for similar images and texts based on the embedding of a query image,text or both.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_image_url (str): The URL of the query image.\n",
    "    - text_query(str): a text description of the product you want\n",
    "    - k (int, optional): The number of top similar items to return. Default is 5.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of product IDs for the top k similar items.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def create_search_query(embedding, k):\n",
    "        return {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"function_score\": {\n",
    "                            \"query\": {\n",
    "                                \"knn\": {\n",
    "                                    \"product_image_and_description_embedding\": {\n",
    "                                        \"vector\": embedding,\n",
    "                                        \"k\": k,\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            \"weight\": 1,\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    base64_image = get_base64_image(query_image_url)\n",
    "    query_multi_embedding_array = normalise_embedding(np.array(get_multi_embedding(image_data=base64_image,\n",
    "                                                                                   text_chunks=text_splitter.split_text(clean_and_prepare_text(get_search_description(base64_image,bedrock))),\n",
    "                                                                                   bedrock=bedrock)))\n",
    "    query_multi_embedding = query_multi_embedding_array.tolist()\n",
    "    response = aoss_py_client.search(index=\"product-embeddings-index\", body=create_search_query(query_multi_embedding,k))\n",
    "    return response['hits']['hits']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa1860",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.e Build our search functions <a id='3.e'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2af9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def visual_search(query_image_url:Optional[str]=None, k:Optional[int]=5):\n",
    "    \"\"\"\n",
    "    Visualize the search results along with the query image.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_image (str or bytes): The URL or byte content of the query image.\n",
    "    - search_results (list): A list of product IDs representing the search results.\n",
    "    - records (list): A list of records containing product information.\n",
    "    \"\"\"\n",
    "    def open_image(query_image_url):\n",
    "        if 'sagemaker' in query_image_url:\n",
    "            image_data_base64, success = fetch_image(query_image_url, s3)\n",
    "            if not success:\n",
    "                print(f\"Failed to fetch image for ID: {query_image_url}\")\n",
    "            image_data = base64.b64decode(image_data_base64)\n",
    "            return Image.open(BytesIO(image_data))\n",
    "        \n",
    "        elif 'root' in query_image_url:\n",
    "            return Image.open(query_image_url)\n",
    "        \n",
    "    search_results = search_opensearch_indexes(query_image_url=query_image_url, k=30)\n",
    "    results = [(item['_source']['prodId'],item['_score']) for item in search_results]\n",
    "    sortedlist = aggregate_and_sort_scores(results)\n",
    "    products = []\n",
    "    for product_id in sortedlist[:5]:\n",
    "        item = product_items_map.get(product_id)\n",
    "        image=item['image_url']\n",
    "        products.append((item['product_name'],image))\n",
    "        \n",
    "    total_rows = 2\n",
    "    \n",
    "    query_image = open_image(query_image_url)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(total_rows, 1, 1)\n",
    "    plt.imshow(query_image)\n",
    "    plt.axis('off')\n",
    "        \n",
    "\n",
    "    # Display Search Results\n",
    "    for i,result in enumerate(products,1):\n",
    "        img = open_image(result[1])\n",
    "        plt.subplot(total_rows, len(products), len(products) + i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"ID: {result[0]}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278b8a3-7fd5-4fe0-bc6c-02c9ecf4fb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test using a sample query image. replace /query_image.jpeg with your actual image path\n",
    "query_image_path = \"/query_image.jpeg\"\n",
    "visual_search(query_image_url=query_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1110fb-586e-4c1f-90f3-ea8339f69a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test with an array of query images. replace /query_image_1.jpeg and /query_image_2.jpeg with your test images\n",
    "paths =[\n",
    "    \"/query_image_1.jpeg\",\n",
    "    \"/query_image_2.jpeg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772ad72-c1db-42ff-ad3c-98cc06930f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    visual_search(query_image_url=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, Amazon OpenSearch Serverless, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: What is the difference between OpenSearch, Amazon OpenSearch Serverless, and Amazon OpenSearch Service?**\n",
    "\n",
    "OpenSearch is a fully open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. For more information, see the [OpenSearch documentation](https://opensearch.org/docs/latest/).\n",
    "\n",
    "Amazon OpenSearch Service provisions all the resources for your OpenSearch cluster and launches it. It also automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures. You can scale your cluster with a single API call or a few clicks in the console.\n",
    "\n",
    "Amazon OpenSearch Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. Serverless removes the operational complexities of provisioning, configuring, and tuning your OpenSearch clusters. It's a good option for organizations that don't want to self-manage their OpenSearch clusters, or organizations that don't have the dedicated resources or expertise to operate large clusters. With OpenSearch Serverless, you can easily search and analyze a large volume of data without having to worry about the underlying infrastructure and data management.\n",
    "\n",
    "**Q: How does Amazon OpenSearch Serverless manage capacity?**\n",
    "\n",
    "With Amazon OpenSearch Serverless, you don't have to manage capacity yourself. OpenSearch Serverless automatically scales compute capacity for your account based on the current workload. Serverless compute capacity is measured in OpenSearch Compute Units (OCUs). Each OCU is a combination of 6 GiB of memory and corresponding virtual CPU (vCPU), as well as data transfer to Amazon S3. For more information about the decoupled architecture in OpenSearch Serverless, see [How it works](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process).\n",
    "\n",
    "**Q: Will Amazon Bedrock capture and store my data?**\n",
    "\n",
    "Amazon Bedrock doesn't use your prompts and continuations to train any AWS models or distribute them to third parties. Your training data isn't used to train the base Amazon Titan models or distributed to third parties. Other usage data, such as usage timestamps, logged account IDs, and other information logged by the service, is also not used to train the models.\n",
    "\n",
    "Amazon Bedrock uses the fine tuning data you provide only for fine tuning an Amazon Titan model. Amazon Bedrock doesn't use fine tuning data for any other purpose, such as training base foundation models.\n",
    "\n",
    "Each model provider has an escrow account that they upload their models to. The Amazon Bedrock inference account has permissions to call these models, but the escrow accounts themselves don't have outbound permissions to Amazon Bedrock accounts. Additionally, model providers don't have access to Amazon Bedrock logs or access to customer prompts and continuations.\n",
    "\n",
    "Amazon Bedrock doesnt store or log your data in its service logs.\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#models-supported).\n",
    "\n",
    "**Q: What is the difference between On-demand and Provisioned Throughput in Amazon Bedrock?**\n",
    "\n",
    "With the On-Demand mode, you only pay for what you use, with no time-based term commitments. For text generation models, you are charged for every input token processed and every output token generated. For embeddings models, you are charged for every input token processed. A token is comprised of a few characters and refers to the basic unit that a model learns to understand user input and prompt to generate results. For image generation models, you are charged for every image generated.\n",
    "\n",
    "With the Provisioned Throughput mode, you can purchase model units for a specific base or custom model. The Provisioned Throughput mode is primarily designed for large consistent inference workloads that need guaranteed throughput. Custom models can only be accessed using Provisioned Throughput. A model unit provides a certain throughput, which is measured by the maximum number of input or output tokens processed per minute. With this Provisioned Throughput pricing, charged by the hour, you have the flexibility to choose between 1-month or 6-month commitment terms.\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon OpenSearch Serverless pricing - go [here](https://aws.amazon.com/opensearch-service/pricing/) and navigate to the <i>Serverless</i> section.\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
